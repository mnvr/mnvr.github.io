<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">

  <title>Vector Spaces for Machine Learning | mnvr.in</title>
  <meta name="description" content="Transpose of a vector is a covector got you confused? No longer">
  <link rel="alternate" type="application/rss+xml" href="rss.xml">

  <style>
    :root {
      color-scheme: light dark;
    }

    body {
      font-family: system-ui, sans-serif;
      margin: 0.8lh;
      line-height: 1.5;
    }

    header,
    main,
    footer {
      max-width: 60em;
    }

    h2 span.muted {
      opacity: 0.55;
    }

    footer cite {
      opacity: 0.7;
    }

    footer time {
      opacity: 0.7;
    }

    blockquote {
      opacity: 0.8;
    }

    hr {
      margin-block: 2em;
      opacity: 0.3;
    }

    .m {
      border: 1px dashed color-mix(in srgb, currentColor 12%, transparent);
      padding: 0.2em;
    }

    svg {
      max-width: 100%;
    }
  </style>

</head>

<body>
  <header>
    <h1>Vector Spaces for Machine Learning</h1>
  </header>

  <main>
    <h2><span class="muted">Layer 0 - </span> Vector space</h2>

    <p>
      A vector space over some scalar field <span class="m">F</span> is a set of objects (“vectors”) that is closed under addition, scaling, and combinations thereof.
    </p>

    <p>
      Commonly the scalar field under consideration is the set of real numbers, <span class="m">ℝ</span>. So the scaling factors (“scalars”) will be real numbers. But the vectors themselves have no inherent association with real numbers. They are arbitrary objects whose set is closed under addition and scalar multiplication, where we get to define what addition and scalar multiplication means.
    </p>

    <p>
      With a reminder that the scalar field can also be non-real, like <span class="m">ℂ</span>, in what follows we’ll implicitly assume that the field we’re using is <span class="m">ℝ</span>.
    </p>

    <hr>

    <h2><span class="muted">Layer 1 - </span>Linear functionals</h2>

    <p>
      For any vector space we can define functions that take elements of that vector space as input and produce the scalar as output.
    </p>

    <p>
      Any arbitrary function, <span class="m">f(x) = a</span>, where <span class="m">x ∈ V</span> and <span class="m">a ∈ ℝ</span>.
    </p>

    <p>
      Now, let us consider only those functions that obey linearity, that is
    </p>

    <p>
      <span class="m">f(ax + by) = a f(x) + b f(y)</span> where <span class="m">x, y ∈ V</span> and <span class="m">a, b ∈ ℝ</span>
    </p>

    <p>
      Such a function is called a linear functional.
    </p>

    <blockquote>
      <p>
        A broader concept is a linear map, which is the set of all linear functions between any two vector spaces. When the target is the scalar field associated with the vector space, then we get a linear functional. “Functional” here just means a function that returns a number (or as we’re referring to them, a scalar).
      </p>
    </blockquote>

    <p>
      Now let us consider the set of possible such linear maps that exist for a given vector space. This set is called the set of linear functionals over <span class="m">V</span>, and can be defined as
    </p>

    <p>
      <span class="m">V* = { f : V → ℝ | f is linear }</span>
    </p>

    <p>
      This <span class="m">V*</span> is another vector space, where the elements are linear functionals. For this set, we can define addition and scalar multiplication as:
    </p>

    <p>
      <span class="m">(f + g)(v) = f(v) + g(v)</span><br>
      <span class="m">(c f)(v) = c f(v)</span>
    </p>

    <p>
      This set is called the “dual space” <span class="m">V*</span> of the original vector space <span class="m">V</span>, and the elements of this dual space (i.e. the linear functionals) are called covectors.
    </p>

    <p>
      Semantically, at this point we can think that
    </p>

    <ul>
      <li>vectors exist</li>
      <li>covectors “probe” vectors</li>
    </ul>

    <p>
      Where “probe” means produce a scalar value to describe a vector. Reminder that at this point, don’t think of a vector as an array of coordinates. We don’t know what a vector is beyond the properties it follows. The covector is a way to collapse a vector down to a scalar, and do it in a way that the collapses follow linearity.
    </p>

    <p>
      Also, note that at this point
    </p>

    <ul>
      <li>vectors do <strong>not</strong> probe / measure vectors</li>
    </ul>

    <hr>

    <h2><span class="muted">Layer 2 - </span>Canonical pairing</h2>

    <p>
      Consider all pairs <span class="m">(f, v)</span> where <span class="m">f ∈ V*</span> and <span class="m">v ∈ V</span>.
    </p>

    <p>
      Now given the primitives we have so far, what could this pairing mean?
    </p>

    <p>
      It can only mean one thing – function application.
    </p>

    <p>
      Let’s think about this. v is an arbitrary object. f is a function that takes vectors and returns scalars in a way that respects linearity. Without assuming any more structure, the only (and perfectly natural) way to combine them is to take <span class="m">(f, v)</span> to mean <span class="m">f(v)</span>.
    </p>

    <p>
      <span class="m">(f, v) ↦ f(v)</span>
    </p>

    <p>
      So we have another set <span class="m">V* × V</span> and a pairing <span class="m">V* × V → ℝ</span>.
    </p>

    <p>
      To reiterate, there exists for every vector space a dual space of covectors, and a canonical pairing
    </p>

    <p>
      <span class="m">(f, v) ↦ f(v)</span>
    </p>

    <p>
      Which <em>is</em> function application.
    </p>

    <p>
      Note so far that we haven’t assumed any extra structure apart from closure under addition and scaling for the vector space. Everything else has been definitional, with the slightly curious but still not very surprising emergence that the dual space of <span class="m">V</span> is also a vector space.
    </p>

    <h3>Aside</h3>

    <p>
      Let us consider an example. Again, note that vectors are arbitrary things, but for the sake of visualization consider vectors as elements of <span class="m">ℝ²</span>. Then
    </p>

    <ul>
      <li>Vectors are arrows;</li>
      <li>Covectors are a family of parallel lines, equally spaced, each line is a same-value contour;</li>
      <li>When a covector acts on a vector, in our example vector space, what it probes is the count of the number of contour lines we cross.</li>
    </ul>


    <svg width="400" height="300" viewBox="0 0 400 300" xmlns="http://www.w3.org/2000/svg">
      <defs>
        <marker id="arrow" viewBox="0 0 10 10" refX="10" refY="5" markerWidth="6" markerHeight="6" orient="auto-start-reverse">
          <path d="M 0 0 L 10 5 L 0 10 z" fill="#CB1919" />
        </marker>
        <rect id="marker" width="6" height="6" x="-3" y="-3" fill="currentColor" />
      </defs>

      <g stroke="currentColor" style="opacity: 0.1" stroke-width="1">
        <path d="M25 35 h360 M25 95 h360 M25 155 h360 M25 215 h360 M25 275 h360" />
        <path d="M25 35 v240 M85 35 v240 M145 35 v240 M205 35 v240 M265 35 v240 M325 35 v240 M385 35 v240" />
      </g>

      <g stroke="#696AC4" stroke-width="2" stroke-linecap="round">
        <line x1="25" y1="215" x2="145" y2="275" />
        <line x1="25" y1="155" x2="265" y2="275" />
        <line x1="25" y1="95" x2="385" y2="275" />
        <line x1="25" y1="35" x2="385" y2="215" />
        <line x1="145" y1="35" x2="385" y2="155" />
        <line x1="265" y1="35" x2="385" y2="95" />
      </g>

      <path d="M25 35 V275 H385" stroke="currentColor" style="opacity: 0.6" stroke-width="2.5" fill="none" />

      <line x1="25" y1="275" x2="145" y2="95" stroke="#CB1919" stroke-width="3" marker-end="url(#arrow)" />

      <use href="#marker" x="55" y="230" />
      <use href="#marker" x="85" y="185" />
      <use href="#marker" x="115" y="140" />
      <use href="#marker" x="145" y="95" /> <text x="145" y="80" fill="#E62323" font-family="serif" font-style="italic" font-size="20">v</text>
      <text x="345" y="185" fill="#696AC4" font-family="serif" font-style="italic" font-size="20">v*</text>
    </svg>

    <p>
      In this geometric picture, assume we were to stretch the x-axis by 2. Neither the vector itself – the arrow – nor the probe done by the covector – the number of crossings – should change because a vector space doesn’t have any concept of coordinates so far (we haven’t come to that yet, we’re only visualizing it that way).
    </p>

    <p>
      To keep both the vector and number of crossings unchanged, we change the grid coordinates the vector’s arrow is labelled by (reduce them), and the spacing between the contour lines (increase them). The end result – the number of crossings – then remains the same.
    </p>

    <p>
      This is why the components (“coordinates”) of the vector are called contravariant (they go against the basis (“grid”) change) while the components of the covector are called covariant (they go with the basis change).
    </p>

    <p>
      But again, a reminder. Vectors are opaque objects; the concept of component hasn’t come yet (and when it does come, it applies to only particular types of vector spaces).
    </p>

    <hr>

    <h2><span class="muted">Layer 3 - </span>Inner product / metric</h2>

    <p>
      This is the first extra structure we add. We choose a map <span class="m">g : V × V → ℝ</span> that satisfies three properties:
    </p>

    <ul>
      <li>symmetry: <span class="m">g(u, v) = g(v, u)</span></li>
      <li>bilinearity: <span class="m">g(ax + by, z) = a g(x, z) + b g(y, z)</span> and <span class="m">g(z, ax + by) = a g(z, x) + b g(z, y)</span></li>
      <li>positive definiteness: <span class="m">g(v, v) = 0 ⇔ v = 0</span>, otherwise <span class="m">g(v, v) &gt; 0</span></li>
    </ul>

    <p>
      Any map that satisfies these criteria is called the “inner product”.
    </p>

    <p>
      Conventionally, these inner products are written as <span class="m">⟨·, ·⟩</span>. That is, instead of <span class="m">g(u, v)</span>, we write <span class="m">⟨u, v⟩</span>.
    </p>

    <p>
      Instead of considering the inner product as a map <span class="m">V × V → ℝ</span>, we can also think of it as a curried function. Fix some vector <span class="m">v</span>, then <span class="m">⟨v, ·⟩</span> is a function of vectors. Since the output is a real number, instead of a function we can give it the more specific terminology – functional. And one of the properties of the inner product is that it is bilinear. So effectively, <span class="m">⟨v, ·⟩</span> is a linear functional.
    </p>

    <p>
      But we already know that the set of all linear functionals associated with the vector space is <span class="m">V*</span>. So <span class="m">⟨v, ·⟩</span> is a member of the dual space, a covector!
    </p>

    <p>
      So any inner product is a particular way of mapping a vector to a covector. Once we have a covector, we can then use it to probe other vectors as usual.
    </p>

    <p>
      In effect, if we have an inner product defined on our vector space, we allow a vector to measure (“probe”) another vector. This allows us to induce a “metric” on the vector space.
    </p>

    <p>
      To expand on this – previously, only covectors could measure (“probe”) a vector, and covectors and vectors live in separate spaces. So while we had a way to “measure” vectors
    </p>

    <p>
      <span class="m">f(v) ∈ ℝ</span>
    </p>

    <p>
      It was not a metric, but just an evaluation, since the value depends on an arbitrary choice of <span class="m">f</span>, each giving incompatible measurements.
    </p>

    <p>
      In contrast, the inner product allows us to induce a metric on the vector space since it gives a natural way to identify the covector for any vector.
    </p>

    <p>
      <span class="m">v ↦ ⟨v, ·⟩ ∈ V*</span>
    </p>

    <blockquote>
      <p>
        Natural here means that once we decide on a particular inner product, then there is only one canonical way to use that inner product to identify the unique covector associated to any vector.
      </p>
    </blockquote>

    <p>
      Identifying vectors with covectors removes the need for external probes. Each vector can now measure itself (giving a norm) and measure any other vector (giving alignment / similarity). Because the inner product is symmetric, this mutual measurement is symmetric: <span class="m">⟨u, v⟩ = ⟨v, u⟩</span>.
    </p>

    <p>
      Going into more details, a metric on a set is a function
    </p>

    <p>
      <span class="m">d : X × X → ℝ</span>
    </p>

    <p>
      That satisfies (for <span class="m">x, y, z ∈ X</span>):
    </p>

    <ul>
      <li>non-negativity: <span class="m">d(x, y) ≥ 0</span></li>
      <li>identity of indiscernibles (distinct points have non-zero distance): <span class="m">d(x, y) = 0 ⇔ x = y</span></li>
      <li>symmetry: <span class="m">d(x, y) = d(y, x)</span></li>
      <li>triangle inequality: <span class="m">d(x, z) ≤ d(x, y) + d(y, z)</span></li>
    </ul>

    <p>
      If we have an inner product <span class="m">⟨·, ·⟩</span>, we can define norm (“length”)
    </p>

    <p>
      <span class="m">‖v‖ = √⟨v, v⟩</span>
    </p>

    <p>
      And distance
    </p>

    <p>
      <span class="m">d(u, v) = ‖u − v‖</span>
    </p>

    <p>
      It can be shown that this d will satisfy the above axioms; thus the inner product allows us to define this metric on the vector space.
    </p>

    <ul>
      <li>we can measure the “size” / “length” (the norm) of a vector by taking its inner product with itself</li>
      <li>we can measure the distance between two vectors (as the norm of the algebraic difference between them)</li>
      <li>we can measure the “alignment” / “similarity” / “angle” between two vectors (as their inner product)</li>
    </ul>

    <p>
      Note that so far we’ve not chosen any coordinate space, basis vectors, or geometric embedding. We start with Layer 0 (vector space), Layer 1 (dual space of linear functionals), Layer 2 (function application as the natural interpretation of pairing vectors and linear functionals). A vector space can remain at this point without any notion of geometry.
    </p>

    <p>
      However, if we additionally choose an appropriate inner product, we get:
    </p>

    <ul>
      <li>inner product → norm</li>
      <li>norm → metric</li>
      <li>metric → distance</li>
      <li>inner product → angle / projection</li>
      <li>self-pairing → length</li>
    </ul>

    <p>
      Geometric concepts (angle, distance, length) emerge, but none of these presuppose that vectors are living in a geometric space.
    </p>

    <p>
      In particular, note that we now have the ability to measure if two vectors are orthogonal to each other – this is true when their inner product is zero.
    </p>

    <hr>

    <h2>Dot product</h2>

    <p>
      The dot product is a particular inner product <span class="m">⟨u, v⟩</span> that can be defined on any vector space that is isomorphic to <span class="m">ℝⁿ</span>.
    </p>

    <p>
      Such vector spaces are also called finite-dimensional real vector spaces. In such spaces, we can choose a set of basis vectors, and then express any other arrow (vector) in the space in terms of scaled combinations (aka linear combinations) of those basis vectors. These scaling factors (coefficients of the linear combination) are the “coordinates” of the arrow (i.e. the vector).
    </p>

    <blockquote>
      <p>
        Note that the concept of inner product doesn’t depend on a choice of basis, but the dot product does.
      </p>
    </blockquote>

    <p>
      In particular, if we choose an orthonormal set of basis (where the orthogonality and norm itself is defined in terms of the inner product – vectors whose inner product is 0 are orthogonal and a set of mutually orthogonal ones which all have a norm of 1 form an orthonormal basis), then that particular inner product, which gets called the dot product, <span class="m">u · v</span> has a simple “sum of pairwise coordinate products” formula
    </p>

    <p>
      <span class="m">u · v = Σ uᵢ vᵢ</span>
    </p>

    <p>
      <span class="m">⟨u, v⟩</span> says “inner product is being used (abstract, basis-independent)”
    </p>

    <p>
      <span class="m">u · v</span> says “Euclidean dot product in an orthonormal coordinate system”
    </p>

    <blockquote>
      <p>
        Note that the concept of inner product doesn’t depend on a choice of basis. The dot product <em>is</em> an inner product so it also doesn’t depend on a choice of basis. However, the dot product <em>formula</em> does require that the coordinates being used are in terms of an orthonormal basis.
      </p>
    </blockquote>

    <hr>

    <h2>ML</h2>

    <p>
      In ML contexts, usually the vector space we deal with is isomorphic to <span class="m">ℝⁿ</span>, the basis are assumed to be the standard orthonormal ones, and the inner product is assumed to be the dot product.
    </p>

    <p>
      The interesting stuff starts happening atop this structure.
    </p>

    <p>
      The dot product is treated as “multiplication” – a binary operation that returns a scalar. Note that the vector space itself has no multiplication primitive; the only operations we get are addition, scaling, and probing using a linear functional from the dual space. Since we’ve chosen an inner product, we also get norm, angle, and distance.
    </p>

    <p>
      So multiplying a vector with another vector is a shorthand for saying take the inner product of the two vectors, in these cases, the dot product. This stands in for the following operations:
    </p>

    <ul>
      <li>transform one of the vectors into its covector</li>
      <li>probe the other vector using this covector</li>
      <li>the resultant scalar is the result of the operation</li>
    </ul>

    <p>
      Matrix–vector multiplication (in this vernacular) is then performing multiple such multiplications, and returning the stacked result. In fact, the matrix–vector multiplication can be (in this vernacular) considered as a primitive – taking multiple dot products of a single vector in one go – and the vector–vector product (most correctly, the inner product) is a special case where it devolves into a single dot product.
    </p>

    <p>
      In such framings, we also start to think in terms of the low-level “coordinates” – vectors as arrays of numbers – instead of thinking of them as opaque objects in a vector space. To retain “type safety” (i.e. we don’t willy-nilly mix vectors and covectors), the mechanical manipulation of coordinates has to make sure that the “shapes” match.
    </p>

    <p>
      There are two operations then:
    </p>

    <ul>
      <li><span class="m">u · v</span> – a dot product. The dimensions of <span class="m">u</span> and <span class="m">v</span> must match. The result is the sum of the pairwise products.</li>
      <li><span class="m">u v</span> – a matrix multiplication, where each “matrix” is either a vector (one-dimensional array of scalars) or a matrix (a two-dimensional array of scalars). For this operation, the inner dimensions must match. So <span class="m">u ∈ ℝ<sup>a×b</sup></span> and <span class="m">v ∈ ℝ<sup>b×c</sup></span> can be multiplied, for any <span class="m">a, b, c ∈ ℕ</span>. The mechanical operation then is to perform <span class="m">a × c</span> dot products of the <span class="m">b</span>-dimensional vectors.</li>
    </ul>

    <p>
      Let’s take the simplest case. Consider <span class="m">u</span> and <span class="m">v</span> as two vectors of the same dimension <span class="m">d</span>. <span class="m">u · v</span> is clear. But saying <span class="m">u v</span> is not correct, because the shapes don’t match, both <span class="m">u, v ∈ ℝ<sup>1×d</sup></span> (if for our problem we’re representing vectors as “row vectors”) or <span class="m">u, v ∈ ℝ<sup>d×1</sup></span> (if for our problem we’re representing vectors as “column vectors”).
    </p>

    <p>
      To get the shapes to match we need to perform a transpose. Mechanically, the transpose is swapping rows and columns, but the actual operation it is doing in our specific Euclidean vector space with orthonormal basis is converting a vector to its covector (or vice versa).
    </p>

    <p>
      So we instead write <span class="m">uᵀ v</span>, which is often thought of as converting one of the vectors into a “matrix”, but really is taking the covector of the vector <span class="m">u</span> to convert it into a linear functional that can then measure the other <span class="m">v</span> to get us our scalar. All that said, in terms of the mechanical calculations, there is no “conversion to covector” – <span class="m">uᵀ v</span> is exactly the same scalar as <span class="m">u · v</span>.
    </p>

  </main>

  <footer>
    <p>
      <cite>Manav Rathi</cite><br>
      <time>Jan 2026</time>
    </p>
    <a href="index">index</a>
  </footer>

</body>

</html>